import os
import json
from src.data_loader import DataLoader
from src.llm_client import LLMClient
from src.utils import split_markdown_by_tag, extract_mapped_case_ids, get_record_by_id


class AutoTutorialPipeline:
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.config_path = os.path.join(root_dir, "config.yaml")
        self.prompts_dir = os.path.join(root_dir, "prompts")
        self.processed_dir = os.path.join(root_dir, "data", "processed")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.llm = LLMClient(self.config_path)
        
    def load_prompt(self, filename):
        path = os.path.join(self.prompts_dir, filename)
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()

    def run_step1(self):
        print("\n=== Running Step 1: Knowledge Enrichment ===")
        
        # 1. ç¡®ä¿æœ‰ analysis_summary.json
        summary_path = os.path.join(self.processed_dir, "analysis_summary.json")
        if not os.path.exists(summary_path):
            print("[Error] analysis_summary.json not found. Run Data Loader first!")
            return

        # 2. è¯»å–åŸå§‹æ•°æ®
        with open(summary_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
            
        # 3. å‡†å¤‡ Prompt
        prompt_tmpl = self.load_prompt("step1_enrich.txt")
        # å°†æ•´ä¸ª raw_data å¡è¿›å» (å¦‚æœå¤ªé•¿å¯ä»¥åªå¡ records åˆ—è¡¨)
        final_prompt = prompt_tmpl.replace("[INSERT_DATA]", json.dumps(raw_data['records'], indent=2))
        
        # 4. è°ƒç”¨ LLM
        print("[LLM] Sending request... (This may take a while)")
        response = self.llm.chat(final_prompt)
        
        # 5. æ¸…æ´—å¹¶ä¿å­˜ç»“æœ (å»é™¤å¯èƒ½å­˜åœ¨çš„ ```
        cleaned_response = response.replace("```json", "").replace("```", "")
        
        output_path = os.path.join(self.processed_dir, "step1_result.json")
        try:
            # éªŒè¯ä¸€ä¸‹æ˜¯å¦æ˜¯åˆæ³•çš„ JSON
            json_obj = json.loads(cleaned_response)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_obj, f, indent=4)
            print(f"[Success] Step 1 complete. Saved to {output_path}")
        except json.JSONDecodeError:
            print("[Error] LLM did not return valid JSON. Saving raw text instead.")
            with open(output_path + ".txt", 'w', encoding='utf-8') as f:
                f.write(cleaned_response)
    
    def run_step2(self):
        print("\n=== Running Step 2: Adaptive Outline Generation ===")
        
        # 1. è¯»å– Step 1 çš„ç»“æœ
        input_path = os.path.join(self.processed_dir, "step1_result.json")
        if not os.path.exists(input_path):
            print("[Error] step1_result.json not found. Run Step 1 first!")
            return

        with open(input_path, 'r', encoding='utf-8') as f:
            step1_data = json.load(f)
            
        # 2. å‡†å¤‡ Prompt
        prompt_tmpl = self.load_prompt("step2_outline.txt")
        # å°† JSON æ•°æ®æ’å…¥ Prompt
        final_prompt = prompt_tmpl.replace("[INSERT_DATA]", json.dumps(step1_data, indent=2))
        
        # 3. è°ƒç”¨ LLM
        print("[LLM] Generating Outline... (Thinking hard)")
        response = self.llm.chat(final_prompt)
        
        # 4. ä¿å­˜ Markdown ç»“æœ
        output_path = os.path.join(self.processed_dir, "step2_outline.md")
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(response)
            
        print(f"[Success] Step 2 complete. Saved to {output_path}")
        
        # ç®€å•éªŒè¯ä¸€ä¸‹æ ‡è®°æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢ Step 3 æŠ¥é”™
        if "<!-- CHAPTER_START -->" not in response:
            print("[Warn] âš ï¸ Generated outline is missing '<!-- CHAPTER_START -->' tags. Step 3 may fail.")


    def run_step3(self):
        print("\n=== Running Step 3: Drafting Chapters ===")
        
        # 1. åŠ è½½èµ„æº
        outline_path = os.path.join(self.processed_dir, "step2_outline.md")
        raw_data_path = os.path.join(self.processed_dir, "analysis_summary.json")
        
        if not os.path.exists(outline_path) or not os.path.exists(raw_data_path):
            print("[Error] Missing outline or raw data. Run previous steps first!")
            return

        with open(outline_path, 'r', encoding='utf-8') as f:
            outline_text = f.read()
        
        with open(raw_data_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
            records = raw_data.get('records', [])

        # 2. åˆ‡åˆ†å¤§çº²
        # æˆ‘ä»¬å‡è®¾å¤§çº²ç»“æ„æ˜¯ï¼š[META] [CHAPTER_1] [CHAPTER_2] ... [APPENDIX]
        # ä½¿ç”¨ <!-- CHAPTER_START --> åˆ‡åˆ†
        # æ³¨æ„ï¼šsplit å‡ºæ¥çš„ç¬¬ä¸€éƒ¨åˆ†é€šå¸¸æ˜¯ METAï¼Œåé¢æ‰æ˜¯æ­£æ–‡ç« èŠ‚
        # æˆ‘ä»¬éœ€è¦æ›´ç²¾ç»†çš„é€»è¾‘ï¼š
        
        # ç­–ç•¥ï¼šå…ˆç”¨ CHAPTER_START åˆ‡ï¼Œç¬¬0éƒ¨åˆ†åŒ…å«METAï¼Œæœ€åä¸€éƒ¨åˆ†å¯èƒ½åŒ…å«APPENDIX
        # ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬ç®€å•åœ°å¤„ç†æ‰€æœ‰åŒ…å« "Mapped Case ID" çš„å—ä½œä¸ºæ­£æ–‡ç« èŠ‚
        
        chunks = split_markdown_by_tag(outline_text, "<!-- CHAPTER_START -->")
        
        drafts = []
        chapter_idx = 0
        
        # åŠ è½½ Prompt æ¨¡æ¿
        prompt_tmpl = self.load_prompt("step3_drafting.txt")

        for chunk in chunks:
            # ç®€å•åˆ¤æ–­ï¼šå¦‚æœè¿™ä¸ªå—é‡Œæ²¡æœ‰ "Mapped Case ID"ï¼Œå¯èƒ½å®ƒåªæ˜¯å‰è¨€æˆ–é™„å½•ï¼Œè·³è¿‡æ­£æ–‡ç”Ÿæˆé€»è¾‘
            case_ids = extract_mapped_case_ids(chunk)
            
            if not case_ids:
                print(f"[Skip] Chunk {chapter_idx} has no cases (likely Preface or Appendix).")
                continue
            
            chapter_idx += 1
            # æå–æ ‡é¢˜ (ç¬¬ä¸€è¡Œ)
            title = chunk.strip().split('\n')[0]
            print(f"[Drafting] Chapter {chapter_idx}: {title} (Cases: {case_ids})")
            
            # 3. å‡†å¤‡ Evidence æ•°æ®
            evidence_list = []
            for pid in case_ids:
                rec = get_record_by_id(records, pid)
                if rec:
                    evidence_list.append(rec)
            
            evidence_json = json.dumps(evidence_list, indent=2)
            
            # 4. ç»„è£… Prompt
            final_prompt = prompt_tmpl.replace("{{FULL_BOOK_OUTLINE}}", outline_text) \
                                      .replace("{{CHAPTER_TITLE}}", title) \
                                      .replace("{{CHAPTER_OUTLINE}}", chunk) \
                                      .replace("{{EVIDENCE_JSON}}", evidence_json)
            
            # 5. è°ƒç”¨ LLM
            response = self.llm.chat(final_prompt)
            drafts.append(response)
            
            # å®æ—¶ä¿å­˜æ¯ä¸€ç« 
            with open(os.path.join(self.processed_dir, f"draft_chapter_{chapter_idx}.md"), 'w', encoding='utf-8') as f:
                f.write(response)
                
        print(f"[Success] Generated {len(drafts)} chapters.")
        return drafts

    def run_step4(self):
        print("\n=== Running Step 4: Final Assembly & Polish ===")
        
        # 1. æ”¶é›†æ‰€æœ‰ draft_chapter_*.md
        draft_files = []
        # éå† processed ç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰ draft_chapter_X.md å¹¶æŒ‰æ•°å­—æ’åº
        for f in os.listdir(self.processed_dir):
            if f.startswith("draft_chapter_") and f.endswith(".md"):
                draft_files.append(f)
        
        # æŒ‰ç« èŠ‚å·æ’åº (draft_chapter_1, draft_chapter_2...)
        # è¿™é‡Œå‡è®¾æ–‡ä»¶åæ ¼å¼å›ºå®šï¼Œæå–æ•°å­—è¿›è¡Œæ’åº
        draft_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))
        
        if not draft_files:
            print("[Error] No draft chapters found. Run Step 3 first!")
            return

        # 2. è¯»å–ç« èŠ‚å†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦
        full_chapters_content = []
        summaries = []
        
        for f_name in draft_files:
            path = os.path.join(self.processed_dir, f_name)
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
                full_chapters_content.append(content)
                # æˆªå–å‰ 500 å­—ç¬¦ä½œä¸ºæ‘˜è¦å–‚ç»™ LLMï¼ŒèŠ‚çœ Token
                summaries.append(f"--- {f_name} ---\n{content[:800]}...\n")
        
        # 3. è°ƒç”¨ LLM ç”Ÿæˆ Meta Info (æ ‡é¢˜, å‰è¨€, é™„å½•)
        prompt_tmpl = self.load_prompt("step4_assembly.txt")
        final_prompt = prompt_tmpl.replace("{{CHAPTER_SUMMARIES}}", "\n".join(summaries))
        
        print("[LLM] Finalizing book metadata...")
        response = self.llm.chat(final_prompt)
        
        # 4. è§£æ JSON è¾“å‡º
        try:
            # æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ markdown æ ‡è®°
            clean_json = response.replace("``````", "").strip()
            meta_data = json.loads(clean_json)
            
            title = meta_data.get("book_title", "ABACUS Tutorial")
            preface = meta_data.get("preface_markdown", "")
            appendix = meta_data.get("appendix_markdown", "")
            
        except json.JSONDecodeError:
            print("[Error] LLM failed to return valid JSON in Step 4. Using fallback.")
            title = "ABACUS å®æˆ˜æŒ‡å— (Auto-Generated)"
            preface = "## å‰è¨€\n(ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¡¥å……)"
            appendix = "## é™„å½•\n(ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¡¥å……)"
        
        # 5. ç‰©ç†æ‹¼æ¥å…¨ä¹¦
        final_book_content = f"# {title}\n\n"
        final_book_content += f"{preface}\n\n"
        final_book_content += "---\n\n"
        
        # æ’å…¥æ­£æ–‡
        for chapter_text in full_chapters_content:
            final_book_content += f"{chapter_text}\n\n---\n\n"
            
        # æ’å…¥é™„å½•
        final_book_content += f"{appendix}\n"
        
        # 6. ä¿å­˜æœ€ç»ˆæ–‡ä»¶
        output_dir = os.path.join(self.root_dir, "output")
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        final_path = os.path.join(output_dir, "ABACUS_Tutorial_Final.md")
        with open(final_path, 'w', encoding='utf-8') as f:
            f.write(final_book_content)
            
        print(f"[Success] Book assembled! Saved to: {final_path}")

    def run_all(self):
        """
        ä¸€é”®è¿è¡Œå…¨æµç¨‹ï¼šStep 1 -> Step 4
        """
        print("ğŸš€ Starting AutoTutorial Pipeline...")
        
        # Step 0: æ•°æ®åŠ è½½ (å¯é€‰ï¼Œå¦‚æœç¡®è®¤æ•°æ®å·²å°±ç»ªå¯è·³è¿‡ï¼Œä½†å»ºè®®åŠ ä¸Šä»¥é˜²ä¸‡ä¸€)
        # æ³¨æ„ï¼šéœ€è¦åœ¨å¤´éƒ¨ import DataLoader
        print("\n[Step 0] Checking/Loading Raw Data...")
        # å‡è®¾ raw_data è·¯å¾„åœ¨ config ä¸­é…ç½®äº†ï¼Œæˆ–è€…ç¡¬ç¼–ç 
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å‡è®¾ data/processed/analysis_summary.json å·²ç»ç”± data_loader.py ç”Ÿæˆå¥½äº†
        # å¦‚æœæƒ³é›†æˆå¾—æ›´ç´§å¯†ï¼Œå¯ä»¥åœ¨è¿™é‡Œå®ä¾‹åŒ– DataLoader å¹¶è°ƒç”¨ process()
        summary_path = os.path.join(self.processed_dir, "analysis_summary.json")
        if not os.path.exists(summary_path):
             print("[Error] Raw data summary not found! Please run 'data_loader.py' first.")
             return

        # Step 1: çŸ¥è¯†å›¾è°±æ„å»º
        self.run_step1()
        
        # Step 2: å¤§çº²ç”Ÿæˆ
        self.run_step2()
        
        # Step 3: æ­£æ–‡æ’°å†™ (æœ€è€—æ—¶)
        drafts = self.run_step3()
        if not drafts:
            print("[Error] Step 3 failed to generate drafts. Stopping.")
            return

        # Step 4: ç»„è£…ç»ˆç¨¿
        self.run_step4()
        
        print("\nğŸ‰ Pipeline Finished Successfully! Check the 'output' folder.")


if __name__ == "__main__":
    pipe = AutoTutorialPipeline(r"C:\MyCode\AutoTutorial")
    #pipe.run_step1()
    #pipe.run_step2()
    #pipe.run_step3()
    #pipe.run_step4()

    # åˆå§‹åŒ– Pipeline
    # ä½¿ç”¨æ‚¨çš„é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    pipe = AutoTutorialPipeline(project_root)
    
    # ä¸€é”®å¯åŠ¨ï¼
    pipe.run_all()

